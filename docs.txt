================================================================================
RESIDENCY SIGNALING SIMULATION DOCUMENTATION
================================================================================

FILE: simulation.py
PURPOSE: Simulate residency matching process with preference signaling
ALGORITHM: Applicant-proposing deferred acceptance (NRMP-style stable matching)

================================================================================
OVERVIEW
================================================================================

This simulation models the residency matching process where:
1. Applicants apply to residency programs (some applications include signals)
2. Programs review applications and create rank lists (prioritizing signals)
3. A stable matching algorithm pairs applicants with programs
4. Results track match success rates and program workload

The simulation tests how varying the number of signals (0-30) affects:
- Number of unmatched applicants
- Number of unfilled program spots
- Average application reviews per program

================================================================================
CONSTANTS AND CONFIGURATION
================================================================================

Input File: constants.csv
Required Columns:
- Variable: Name of the constant
- Value (Must be INTEGERS): Integer value
- Description: Human-readable explanation (dropped during loading)

Key Constants:
- n_programs: Total number of residency programs (e.g., 320)
- n_applicants: Total number of applicants (e.g., 1000)
- spots_per_program: Positions available per program (e.g., 4)
- interviews_per_spot: Interview slots per position (e.g., 3)
  * Total interviews = spots_per_program × interviews_per_spot
- max_applications: Maximum applications each applicant can send (e.g., 30)
- simulations_per_s: Number of times to run each signal value (for statistics)
- study_min_signal: Minimum signal value to test (typically 0)
- study_max_signal: Maximum signal value to test (typically 30)

All values are automatically converted to integers during loading.

================================================================================
QUARTILE SYSTEM
================================================================================

Both applicants and programs are divided into 4 equal quartiles:
- Quartile 1: Top tier (IDs 0 to n/4-1)
- Quartile 2: Upper-middle tier (IDs n/4 to n/2-1)
- Quartile 3: Lower-middle tier (IDs n/2 to 3n/4-1)
- Quartile 4: Bottom tier (IDs 3n/4 to n-1)

Quartiles represent quality/competitiveness tiers:
- Lower ID = higher quality/more competitive
- Applicant and program quartiles are independent

Function: get_quartile_dict(n)
- Input: Total number of entities (programs or applicants)
- Output: Dictionary mapping quartile number (1-4) to list of IDs
- Example: {1: [0,1,2,...,79], 2: [80,81,...,159], ...}

================================================================================
APPLICANT CLASS
================================================================================

Class Variables (shared across all applicants):
- n_applications: Total applications each applicant sends (from constants)
- n_applicants: Total number of applicants (from constants)
- n_signals: Number of signaled applications (set per simulation run)
- n_non_signals: Non-signaled applications (n_applications - n_signals)

Instance Variables:
- id: Unique identifier (0 to n_applicants-1)
- quartile: Quality tier (1-4, based on ID)
- matched_program: Program ID where applicant matched (None if unmatched)
- signaled_programs: List of program IDs receiving signals
- non_signaled_programs: List of program IDs receiving non-signaled applications
- final_rank_list: Combined preference list (signaled first, then non-signaled)

Key Methods:

1. update_signal_number(signal_number) - CLASS METHOD
   - Updates n_signals and n_non_signals for all applicants
   - Called once per signal value being tested
   - Ensures signal + non-signal = max_applications

2. get_quartile()
   - Determines applicant's quartile based on ID
   - Lower IDs = better applicants (quartile 1)

3. pick_programs(all_programs, program_quartile_list, signals)
   - Core application distribution logic
   - Creates list of programs to apply to
   - Updates program's received_signals or received_no_signals lists
   
   Application Distribution Strategy (50/25/25 split):
   - 50% applications to own quartile
   - 25% to quartile above (or same if already top)
   - 25% to quartile below (or same if already bottom)
   
   Algorithm:
   a. Divide application count by 4
   b. Remainder goes to own quartile
   c. For each quartile allocation:
      - Randomly select programs from that quartile
      - Ensure no duplicate applications
      - Add applicant to program's signal/non-signal list
   
   Safety Features:
   - Prevents infinite loops (1000 attempt limit)
   - Raises error if not enough unique programs available
   - Validates final application count

4. __init__(id_index, programs, program_quartile_list)
   - Creates applicant instance
   - Generates signaled and non-signaled application lists
   - Sorts both lists in ascending order (by program ID)
   - Combines into final_rank_list (signals prioritized)

================================================================================
PROGRAM CLASS
================================================================================

Class Variables:
- spots_per_program: Positions available (from constants)
- num_interviews: Total interview slots (spots × interviews_per_spot)

Instance Variables:
- id: Unique identifier (0 to n_programs-1)
- quartile: Quality tier (1-4, based on ID)
- received_signals: List of applicant IDs who signaled this program
- received_no_signals: List of applicant IDs who didn't signal
- reviewed_applications: Count of applications program reviews
- final_rank_list: Ordered preference list of applicants (length = num_interviews)
- tentative_matches: Current matches during stable matching algorithm

Key Methods:

1. get_quartile()
   - Determines program's quartile based on ID
   - Lower IDs = more prestigious programs (quartile 1)

2. create_final_rank_list_and_count_reviews()
   - Called after all applicants have applied
   - Creates program's preference ordering
   
   Review Logic:
   a. Always review ALL signaled applications
   b. Sort signals in ascending ID order (prefer better applicants)
   c. If signals ≥ interview slots:
      - Fill all slots with top signals
      - reviewed_applications = len(received_signals)
   d. If signals < interview slots:
      - Use all signals
      - Review ALL non-signals to fill remaining slots
      - Sort non-signals in ascending ID order
      - reviewed_applications = len(signals) + len(non-signals)
   
   Priority: Signaled applicants always ranked before non-signaled
   
   Note: Current implementation reviews ALL non-signals if needed.
         Future optimization could use greedy parallel selection.

3. __init__(id_index)
   - Creates program instance
   - Initializes empty application lists
   - Prepares for receiving applications

================================================================================
STABLE MATCHING ALGORITHM
================================================================================

Function: stable_match(applicants, programs)

Implementation: Applicant-proposing deferred acceptance (Gale-Shapley)
- Produces stable matching (no blocking pairs)
- Applicant-optimal (best possible stable outcome for applicants)
- Strategy-proof for applicants (truth-telling is optimal)

Algorithm Steps:

1. INITIALIZATION
   - free_applicants: Set of unmatched applicant IDs
   - next_proposal_index: Tracks next program on each applicant's list
   - All programs start with empty tentative_matches

2. PROPOSAL PHASE (while free applicants exist)
   a. Select a free applicant with programs remaining on their list
   b. Applicant proposes to next program on their final_rank_list
   c. Increment that applicant's next_proposal_index
   
   d. Program evaluates proposal:
      - If applicant not on program's final_rank_list: REJECT (continue loop)
      - Otherwise: Add to tentative_matches
   
   e. Program sorts tentative matches by preference (rank_list order)
      - Lower index in program's final_rank_list = more preferred
      
   f. Program keeps only top spots_per_program applicants:
      - If more than capacity: reject excess applicants
      - Rejected applicants return to free_applicants pool
      - Accepted applicant removed from free_applicants

3. TERMINATION
   - Loop ends when no free applicant has programs left to propose to
   - Some applicants may remain unmatched (exhausted rank lists)
   - Some programs may remain unfilled (didn't attract enough acceptable applicants)

4. FINALIZATION
   - For each program's tentative_matches:
   - Set matched_program for those applicants

Output: (applicants, programs) with updated match information

Stability Guarantee:
- No applicant-program pair would both prefer each other over current match
- Once achieved, no individual has incentive to deviate

================================================================================
UTILITY FUNCTIONS
================================================================================

1. count_unmatched(applicants, programs)
   - Counts applicants where matched_program is None
   - Sums unfilled spots: (spots_per_program - len(tentative_matches))
   - Returns: (unmatched_applicants, unfilled_spots)

2. get_quartile_dict(n)
   - Divides n entities into 4 equal quartiles
   - Returns dictionary: {quartile_number: [list of IDs]}

3. print_constants()
   - Displays simulation parameters in readable format
   - Called at start of main execution

================================================================================
SIMULATION EXECUTION FLOW
================================================================================

Function: run_simulation(s)

Steps:
1. Create quartile dictionary for programs
2. Instantiate all Program objects (n_programs)
3. Instantiate all Applicant objects (n_applicants)
   - Each applicant picks programs and sends applications
   - Programs receive signals and non-signals
4. Programs create final rank lists and count reviews
5. Calculate total reviews across all programs
6. Run stable matching algorithm
7. Count unmatched applicants and unfilled spots
8. Return: (unmatched_applicants, unfilled_spots, reviews_per_program)

================================================================================
MAIN EXECUTION BLOCK
================================================================================

When run as main script:

1. SETUP
   - Print simulation constants
   - Initialize result dictionaries for each signal value
   - Three metrics tracked: unmatched applicants, unfilled spots, reviews

2. NESTED LOOPS
   Outer loop: For each signal value (study_min_signal to study_max_signal)
   - Update Applicant class signal counts
   - Print progress
   
   Inner loop: Run simulations_per_s repetitions
   - Execute run_simulation()
   - Append results to appropriate dictionary

3. DATA PROCESSING
   - Convert results to DataFrames (columns = signal values, rows = runs)
   - Add 'Parameter' column to identify metric
   - Concatenate all DataFrames vertically

4. OUTPUT FILES
   Generated CSV files:
   
   a. simulation_results.csv (COMBINED)
      - All three metrics in one file
      - Parameter column identifies metric type
      - Easy to load for analysis
   
   b. results_unmatched_applicants.csv
      - Only unmatched applicant data
      - Kept for backward compatibility
   
   c. results_unfilled_spots.csv
      - Only unfilled spot data
      - Kept for backward compatibility
   
   d. results_reviews_per_program.csv
      - Only review count data
      - Kept for backward compatibility

================================================================================
DATA STRUCTURE
================================================================================

Result Dictionaries Structure:
{
    0: [run1_value, run2_value, ..., runN_value],
    1: [run1_value, run2_value, ..., runN_value],
    ...
    30: [run1_value, run2_value, ..., runN_value]
}

Where:
- Keys: Signal values (0 to 30)
- Values: List of results from multiple simulation runs
- Length of each list: simulations_per_s

DataFrame Structure:
- Rows: Individual simulation runs
- Columns: 'Parameter' + signal values (0, 1, 2, ..., 30)
- Each parameter type has simulations_per_s rows

Combined DataFrame:
- Unmatched_Applicants rows (simulations_per_s)
- Unfilled_Spots rows (simulations_per_s)
- Reviews_Per_Program rows (simulations_per_s)
- Total rows: 3 × simulations_per_s

================================================================================
KEY ASSUMPTIONS AND LIMITATIONS
================================================================================

1. APPLICANT BEHAVIOR
   - Truth-telling in rank list creation
   - Random program selection within quartile constraints
   - No strategic signaling decisions

2. PROGRAM BEHAVIOR
   - Must review all signaled applications
   - Currently reviews all non-signals if signals don't fill interview slots
   - Preferences based solely on applicant ID (quality proxy)

3. QUARTILE SYSTEM
   - Assumes n_programs and n_applicants divisible by 4
   - ID directly correlates with quality

4. APPLICATION LIMITS
   - Requires max_applications ≤ programs_per_quartile
   - With 50/25/25 split, max 50% of applications to one quartile
   - Example: 320 programs → 80 per quartile → max ~40 applications safe

5. MATCHING PROPERTIES
   - Stable but not necessarily Pareto optimal
   - Applicant-optimal among stable matchings
   - Some applicants/programs may remain unmatched

6. RANDOMNESS
   - Program selection uses numpy.random.choice
   - Results vary across runs (hence multiple simulations)
   - Set random seed for reproducibility if needed

================================================================================
COMPUTATIONAL COMPLEXITY
================================================================================

Time Complexity (per simulation):
- Applicant creation: O(n_applicants × max_applications)
- Program rank list creation: O(n_programs × applications_per_program)
- Stable matching: O(n_applicants × max_applications) worst case
- Overall: O(n_applicants × max_applications)

Space Complexity:
- O(n_applicants + n_programs + total_applications)

Typical Run Time (estimated):
- With n_applicants=1000, n_programs=320, max_applications=30:
- Single simulation: ~1-5 seconds
- Full study (31 signal values × simulations_per_s): variable
- Example: 31 × 100 = 3100 simulations → ~1-2 hours

Memory Usage:
- Moderate (all objects kept in memory during simulation)
- Results stored cumulatively (3 metrics × signal_values × simulations_per_s)

================================================================================
ERROR HANDLING
================================================================================

Errors Raised:
1. RuntimeError in pick_programs():
   - Infinite loop detection (>1000 attempts)
   - Indicates impossible program selection constraints
   - Solution: Increase n_programs or decrease max_applications

Warning Messages:
1. Applicant application count mismatch
   - Expected vs actual application count differs
2. Program rank list length mismatch
   - Expected num_interviews vs actual length

These warnings indicate potential logic errors but don't halt execution.

================================================================================
FUTURE ENHANCEMENTS
================================================================================

Potential Optimizations:
1. Greedy parallel program review selection (vs reviewing all non-signals)
2. Vectorized operations for applicant/program creation
3. Multiprocessing for parallel simulation runs
4. Caching/memoization for repeated calculations

Potential Features:
1. Variable application distributions (not just 50/25/25)
2. Program capacity variations
3. Couples matching
4. Preference lists with ties
5. Regional/geographic constraints
6. Different signal allocation mechanisms

================================================================================
USAGE EXAMPLE
================================================================================

To run simulation:

1. Prepare constants.csv with required parameters
2. Run: python simulation.py
3. Monitor progress (prints signal value and simulation number)
4. Results saved to CSV files automatically

To modify parameters:
- Edit constants.csv
- Rerun simulation
- Previous results will be overwritten

To analyze results:
- Use data_analysis.py to generate plots
- Or load CSV files into preferred analysis tool

================================================================================
RELATED FILES
================================================================================

1. constants.csv
   - Input file with simulation parameters
   - Required before running

2. data_analysis.py
   - Reads simulation_results.csv or individual result files
   - Generates visualization plots
   - Calculates statistics (means, confidence intervals)

3. Output CSV files
   - simulation_results.csv (combined)
   - results_unmatched_applicants.csv
   - results_unfilled_spots.csv
   - results_reviews_per_program.csv

================================================================================
REFERENCES
================================================================================

Stable Matching Theory:
- Gale, D. and Shapley, L.S., 1962. "College admissions and the stability 
  of marriage." The American Mathematical Monthly, 69(1), pp.9-15.

NRMP Background:
- National Resident Matching Program (NRMP) uses applicant-proposing 
  deferred acceptance algorithm
- Preference signaling introduced to reduce application review burden

Theoretical Properties:
- Strategy-proofness for applicants (Roth, 1982)
- Stability guarantees (Gale-Shapley, 1962)
- Applicant-optimality among stable matchings

================================================================================
END OF DOCUMENTATION
================================================================================




================================================================================
DATA ANALYSIS SCRIPT DOCUMENTATION
================================================================================

FILE: data_analysis.py
PURPOSE: Analyze and visualize results from residency signaling simulations

================================================================================
OVERVIEW
================================================================================

This script reads simulation results from a combined CSV file and generates
visualizations showing how three key metrics vary with the number of signals
used in the residency matching process. For each metric, it calculates:
- Mean values across multiple simulation runs
- 95% confidence intervals to show statistical uncertainty
- Plots showing trends as signal numbers increase

================================================================================
INPUT DATA
================================================================================

Input File: results_combined.csv

Structure:
- Column 1: "Parameter" - identifier for which metric the row represents
- Columns 2-N: Integer column names (0, 1, 2, ..., 30) representing 
  different numbers of signals used
- Each row contains simulation results for one parameter at all signal values
- Multiple rows per parameter represent repeated simulation runs

Expected Parameters:
1. Unmatched_Applicants: Number of applicants who didn't match to any program
2. Unfilled_Spots: Number of program positions that remained empty
3. Reviews_Per_Program: Average number of applications reviewed per program

================================================================================
PROCESSING STEPS
================================================================================

1. DATA LOADING
   - Reads the combined CSV file containing all simulation results
   - CSV must have 'Parameter' column plus numeric columns for signal values

2. INDEX EXTRACTION
   - Extracts column names (excluding 'Parameter')
   - Converts column names to integers (signal values: 0, 1, 2, ..., 30)
   - Sorts to ensure proper x-axis ordering

3. PARAMETER DEFINITION
   - Defines internal parameter names matching the CSV
   - Defines human-readable graph labels:
     * "Number of Unmatched Applicants"
     * "Number of Unfilled Spots"
     * "Average Number of Reviews Per Program"

4. STATISTICAL CALCULATIONS (for each parameter and signal value)
   - Filters data for specific parameter
   - For each signal value:
     a. Extracts all simulation run values
     b. Calculates mean
     c. Calculates standard error of the mean (SEM)
     d. Computes 95% confidence interval using t-distribution
        - Uses degrees of freedom = (number of runs - 1)
        - Confidence interval: mean ± (t-critical × SEM)

5. VISUALIZATION GENERATION
   - Creates two sets of plots:
     a. Combined plot with all three parameters (subplots)
     b. Individual detailed plots for each parameter

================================================================================
OUTPUT FILES
================================================================================

1. simulation_results.png
   - Combined figure with 3 subplots (one per parameter)
   - Size: 10 inches wide × 12 inches tall (4 inches per subplot)
   - Resolution: 300 DPI
   - Shows all three metrics together for comparison

2. unmatched_applicants_plot.png
   - Individual plot for unmatched applicants
   - Size: 10 × 6 inches
   - Resolution: 300 DPI
   - Includes data point markers

3. unfilled_spots_plot.png
   - Individual plot for unfilled program spots
   - Size: 10 × 6 inches
   - Resolution: 300 DPI
   - Includes data point markers

4. reviews_per_program_plot.png
   - Individual plot for reviews per program
   - Size: 10 × 6 inches
   - Resolution: 300 DPI
   - Includes data point markers

================================================================================
PLOT FEATURES
================================================================================

All plots include:
- X-axis: Number of Signals (integer values, not decimal increments)
- Y-axis: Metric value (parameter-specific)
- Blue line: Mean value across simulation runs
- Shaded blue region: 95% confidence interval
- Grid: Light gray gridlines for easier reading
- Legend: Labels for mean line and confidence interval
- Title: Descriptive title with parameter name

Combined Plot Specifics:
- Three vertically stacked subplots
- Shared x-axis concept (Number of Signals)
- Different y-axis scales for each metric

Individual Plot Specifics:
- Larger size for detailed viewing
- Circular markers on the mean line
- Same styling as combined plot for consistency

================================================================================
STATISTICAL METHODOLOGY
================================================================================

Confidence Interval Calculation:
- Uses Student's t-distribution (appropriate for small sample sizes)
- Formula: CI = mean ± (t-critical × SEM)
  where:
  - t-critical: value from t-distribution at 95% confidence level
  - SEM: Standard Error of Mean = std_dev / sqrt(n)
  - degrees of freedom = n - 1 (n = number of simulation runs)

Why 95% Confidence Interval?
- Standard in scientific research
- Means: 95% of the time, the true population mean falls within this range
- Narrower interval = more precise estimate
- Wider interval = more uncertainty

================================================================================
DEPENDENCIES
================================================================================

Required Python packages:
- pandas: Data manipulation and CSV reading
- matplotlib.pyplot: Creating plots and figures
- numpy: Numerical operations
- scipy.stats: Statistical functions (t-distribution, SEM)

Minimum versions: (typically)
- pandas >= 1.0.0
- matplotlib >= 3.0.0
- numpy >= 1.18.0
- scipy >= 1.4.0

================================================================================
USAGE EXAMPLE
================================================================================

To run this script:

1. Ensure results_combined.csv exists in the same directory
2. Run: python data_analysis.py
3. Wait for processing (displays plots interactively)
4. Find output PNG files in the same directory

================================================================================
CUSTOMIZATION OPTIONS
================================================================================

Easy modifications:

1. Change signal range:
   - Modify the column names in results_combined.csv
   - Script automatically detects available signal values

2. Change confidence level:
   - Replace 0.95 in stats.t.interval() with desired level
   - Example: 0.99 for 99% CI, 0.90 for 90% CI

3. Adjust plot appearance:
   - Modify figsize tuples for different dimensions
   - Change colors: replace 'b-' with other color codes
   - Adjust linewidth, alpha (transparency), fontsize values

4. Add more parameters:
   - Add to parameters list
   - Add to parameter_graph_names list
   - Ensure data exists in CSV

================================================================================
NOTES AND LIMITATIONS
================================================================================

1. Assumes CSV column names are integers (or convertible to integers)
2. Requires at least 2 simulation runs per signal value for CI calculation
3. All plots display and are saved (consider commenting out plt.show() 
   for batch processing)
4. Memory usage scales with number of simulations and signal values
5. t-distribution assumption requires normally distributed data
   (valid for large sample sizes by Central Limit Theorem)

================================================================================
END OF DOCUMENTATION
================================================================================